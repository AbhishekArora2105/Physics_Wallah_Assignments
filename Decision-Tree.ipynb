{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a7aa1513-c1ad-4670-b432-3e9257458c6f",
   "metadata": {},
   "source": [
    "Q1: What is a Decision Tree, and how does it work in the context of classification?\n",
    "Ans: A Decision Tree is a non-parametric supervised learning algorithm used for both classification and regression. It models decisions in a hierarchical, flowchart-like structure, consisting of a root node, internal nodes, and leaf nodes. In classification, the algorithm works through recursive partitioning: it repeatedly splits the data at each node based on the feature that yields the most \"pure\" subsets (meaning the samples mostly belong to one class). Purity is measured using metrics like Gini Impurity or Information Gain. This process continues until a stopping criterion is met, and a new data point is classified by following the path of decisions from the root to a leaf, which assigns the class label."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0a7a249-2277-41f1-bb4d-64ca9f52e64c",
   "metadata": {},
   "source": [
    "Q2: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
    "Ans: Gini Impurity\n",
    "Gini Impurity measures the likelihood of a randomly chosen data point being incorrectly classified if it were labeled according to the distribution of classes in its node. Its value is lowest when the node is pure (contains only one class) and highest when the classes are evenly distributed (maximum impurity). The algorithm using Gini Impurity (like CART) aims to find the split that results in the lowest weighted average impurity in the resulting branches, thereby moving toward more class-specific subsets.\n",
    "Entropy and Information Gain\n",
    "Entropy measures the degree of randomness or uncertainty within a node's class distribution. A low Entropy value means the node is very orderly and pure, while a high Entropy value indicates high disorder and a balanced mix of classes. This metric is used to calculate Information Gain (IG), which is the amount of reduction in uncertainty achieved by a split. The algorithm (like ID3 or C4.5) prioritizes the split that yields the maximum Information Gain, because this split provides the most valuable information for distinguishing between the classes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f788843e-bff8-40be-9020-c4dceea3956b",
   "metadata": {},
   "source": [
    "Q3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "Ans: Pre-Pruning, also known as early stopping, is a technique where the growth of the decision tree is halted during the training process. This is achieved by setting constraints (hyperparameters) like a maximum tree depth or a minimum threshold for impurity reduction (Information Gain or Gini Impurity). The key difference is that the tree never grows to its full, potentially overfit, complexity. A practical advantage of Pre-Pruning is computational efficiency: by stopping the tree's growth early, it saves significant time and memory, making it the preferred method for training models on very large datasets.\n",
    "Post-Pruning involves letting the decision tree grow to its maximum possible depth, often resulting in an overfit model, and then after training, trimming back or removing non-significant branches and nodes. This backward process often uses a separate validation dataset to evaluate which subtrees can be replaced by leaf nodes without significantly degrading accuracy. A practical advantage of Post-Pruning is that it usually yields a more accurate or optimal final tree: because it fully explores all possible splits before making a reduction decision, it avoids the risk of prematurely stopping on a suboptimal split that might have led to better subsequent splits."
   ]
  },
  {
   "cell_type": "raw",
   "id": "59a164f7-4948-46aa-863a-1cb067eecc5e",
   "metadata": {},
   "source": [
    "Q4:  What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "Ans: Information Gain (IG) is a core metric in decision tree algorithms that quantifies the expected reduction in entropy (uncertainty or disorder) achieved by splitting a dataset based on a specific feature. It is calculated by taking the difference between the entropy of the parent node and the weighted average entropy of the resulting child nodes. IG is critically important because it serves as the primary greedy criterion for choosing the best split at every internal node of the tree. The decision tree algorithm selects the feature and split point that yields the maximum Information Gain, as this choice is guaranteed to produce the most homogeneous (pure) child subsets, thereby most effectively reducing uncertainty and leading to faster, more accurate classification paths."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ada647bc-960a-41e9-b005-c09edd928d1b",
   "metadata": {},
   "source": [
    "Q5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "Ans: Decision Trees are used widely in fields like Finance (for credit risk), Healthcare (for diagnosis), and Marketing (for churn prediction) because their structure is easy to interpret and visualize, mirroring human logic. Their main advantages are this interpretability and the ability to handle various data types without scaling. However, their core limitations are a high tendency to overfit the training data and significant instability (high variance), meaning small data changes can drastically alter the tree structure. Ensemble methods like Random Forest are often used to overcome these drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e45abfa-f87c-41c3-a248-510abe20f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the trained model is 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Q6: Write a Python program to:\n",
    "# ● Load the Iris Dataset\n",
    "# ● Train a Decision Tree Classifier using the Gini criterion\n",
    "# ● Print the model’s accuracy and feature importances\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data.data , columns = data.feature_names)\n",
    "x = df\n",
    "y = data.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y , test_size=0.3 , random_state=1)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion='gini')\n",
    "classifier.fit(x_train , y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"The accuracy score of the trained model is {accuracy_score(y_test , y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4757a7-d29c-45e6-9833-5b0f930e8073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the trained model is 0.9555555555555556\n",
      "The accuracy score of the full trained model is 0.9555555555555556\n",
      "There is no change in accuracy between both models.\n"
     ]
    }
   ],
   "source": [
    "# Q7: Write a Python program to:\n",
    "# ● Load the Iris Dataset\n",
    "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "# a fully-grown tree.\n",
    "\n",
    "\n",
    "classifier2 = DecisionTreeClassifier(max_depth=3)\n",
    "classifier2.fit(x_train , y_train)\n",
    "y_pred2 = classifier2.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"The accuracy score of the trained model is {accuracy_score(y_test , y_pred2)}\")\n",
    "print(f\"The accuracy score of the full trained model is {accuracy_score(y_test , y_pred)}\")\n",
    "\n",
    "print(\"There is no change in accuracy between both models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3af3a73-dfba-446a-b5f4-ac831754af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of the trained model is 0.48981919017848835\n"
     ]
    }
   ],
   "source": [
    "# Q8: Write a Python program to:\n",
    "# ● Load the California Housing dataset from sklearn\n",
    "# ● Train a Decision Tree Regressor\n",
    "# ● Print the Mean Squared Error (MSE) and feature importances\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(data.data , columns=data.feature_names)\n",
    "df['Price'] =  data.target\n",
    "x = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.3,random_state=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(x_train , y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(f\"The mean squared error of the trained model is {mean_squared_error(y_test , y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b287e34-6149-4b58-b82e-563599856877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "The best estimator is {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2}\n",
      "The accuracy score of the trained model is 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Q9: Write a Python program to:\n",
    "# ● Load the Iris Dataset\n",
    "# ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "# ● Print the best parameters and the resulting model accuracy\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data.data , columns = data.feature_names)\n",
    "x = df\n",
    "y = data.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y , test_size=0.3 , random_state=1)\n",
    "\n",
    "parameter = {\n",
    "    'criterion':['gini','entropy','log_loss'],\n",
    "    'min_samples_split':[1,2,3,4,5,6,7,10],\n",
    "    'max_depth':[1,2,3,4,5,6,7,10]\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = DecisionTreeClassifier()\n",
    "model = GridSearchCV(clf , param_grid=parameter , cv = 5 , scoring = \"accuracy\",verbose = 1)\n",
    "\n",
    "model.fit(x_train , y_train)\n",
    "print(f\"The best estimator is {model.best_params_}\")\n",
    "y_pred = model.best_estimator_.predict(x_test)\n",
    "print(f\"The accuracy score of the trained model is {accuracy_score(y_test , y_pred)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9135b3e-b32c-4839-8f91-f3480ab51fdd",
   "metadata": {},
   "source": [
    " Q10: Imagine you’re working as a data scientist for a healthcare company that\n",
    " wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    " mixed data types and some missing values.\n",
    "\n",
    "Concise Decision Tree Pipeline for Disease Prediction\n",
    "\n",
    "This pipeline outlines the core steps for building a Decision Tree model in a healthcare setting with mixed/missing data, focusing on model reliability and clinical utility.\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "Handling Missing Values: Impute numerical features using the median (robust to outliers) and categorical features using the mode or an explicit 'Missing' label.\n",
    "\n",
    "Encoding: Use Label Encoding for ordinal features and One-Hot Encoding for nominal (unordered) categories to convert all data to a numerical format required by the tree.\n",
    "\n",
    "2. Model Training and Tuning\n",
    "\n",
    "Training: Train the initial DecisionTreeClassifier on a divided dataset (Train, Validation, Test).\n",
    "\n",
    "Hyperparameter Tuning (Pruning): Since Decision Trees easily overfit, tuning is essential. Use Grid Search with Cross-Validation to find the optimal pruning parameters, such as max_depth (pre-pruning) and Cost Complexity Pruning (ccp_alpha, post-pruning).\n",
    "\n",
    "3. Evaluation and Business Value\n",
    "\n",
    "Evaluation Focus: In healthcare, Sensitivity (Recall) is the most critical metric. The model must maximize the correct identification of sick patients (minimize False Negatives). Secondary metrics include Precision, F1-Score, and AUC.\n",
    "\n",
    "Business Value: The Decision Tree is a \"white-box\" model, providing high Clinical Interpretability and Trust . Physicians can trace the exact IF-THEN rules leading to a prediction, facilitating Early Intervention and improving resource allocation by flagging high-risk patients proactively.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
