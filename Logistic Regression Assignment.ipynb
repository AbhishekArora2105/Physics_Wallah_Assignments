{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f807470-bc80-4b64-b2d2-6a0d3dbb9513",
   "metadata": {},
   "source": [
    "Q1: What is Logistic Regression, and how does it differ from Linear\n",
    "Regression?\n",
    "Ans: Logistic Regression is a statistical model primarily used for binary classification problems, meaning it predicts the probability of an event belonging to one of two categories (e.g., Yes/No, 0/1, True/False, Spam/Not Spam).\n",
    "As it contains the word \"regression,\" it is fundamentally a classification algorithm. It works by applying the sigmoid function (also called the logistic function) to the linear combination of input variables. This transformation squashes the output of the linear equation into a probability value between 0 and 1. A threshold (often 0.5) is then applied to this probability to classify the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013a392-78f2-4271-9fd2-ade513440ff7",
   "metadata": {},
   "source": [
    "Q2: Explain the role of the Sigmoid function in Logistic Regression.\n",
    "\n",
    "Ans: The Sigmoid function, also known as the logistic function, is the indispensable component that defines Logistic Regression as a classification algorithm, despite its name. The model first computes a linear combination of its inputs and weights, producing a result that can range from negative to positive infinity. Since this unbounded output cannot directly represent a probability, the Sigmoid function is applied. This function's characteristic S-shaped curve  transforms any real-valued number into an output that is strictly bounded between 0 and 1. This constrained output is interpreted as the predicted probability that the observation belongs to the positive class (Class 1). Finally, this probability is converted into a binary classification (0 or 1) by applying a predetermined threshold (most commonly 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb6115-e4ee-4758-8a8e-39092165beb9",
   "metadata": {},
   "source": [
    "Q3: What is Regularization in Logistic Regression and why is it needed?\n",
    "\n",
    "Ans: Regularization in Logistic Regression is a technique that adds a penalty term to the model's loss function to discourage excessively large coefficient (weight) values.\n",
    "\n",
    "It is needed primarily to prevent overfitting, which occurs when the model learns the noise in the training data too well, leading to poor performance on new data. By forcing weights to be smaller, regularization reduces model complexity and increases the model's ability to generalize.\n",
    "\n",
    "The two main types are:\n",
    "\n",
    "L2 Regularization (Ridge): Shrinks coefficients toward zero but rarely to zero, helping to handle multicollinearity.\n",
    "\n",
    "L1 Regularization (Lasso): Can shrink coefficients exactly to zero, effectively performing automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebefaf-0dc3-4b5e-bc06-5040e7696df5",
   "metadata": {},
   "source": [
    "Q4: What are some common evaluation metrics for classification models, and\n",
    "why are they important?\n",
    "Ans: Classification model evaluation metrics are vital because they offer different views on a model's performance, which is especially critical in imbalanced datasets where simple Accuracy can be misleading.\n",
    "The key metrics, derived from the Confusion Matrix, provide insight into the specific types of errors being made:\n",
    "Accuracy: Overall correct predictions. It's useful for balanced datasets but poor for judging performance when one class greatly outweighs the other.\n",
    "Precision: Focuses on the correctness of positive predictions. It's important when False Positives are costly (e.g., wrongly flagging a safe transaction as fraud).\n",
    "Recall (Sensitivity): Focuses on the model's ability to find all actual positive cases. It's important when False Negatives are costly (e.g., failing to detect a disease).\n",
    "F1 Score: A single metric that represents the balance between Precision and Recall. It's essential for imbalanced datasets because it penalizes models that favor one metric over the other.\n",
    "AUC-ROC: Measures the model's overall ability to distinguish classes across all possible classification thresholds, making it a robust measure for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a36d336-b6ca-411d-a9af-7b4f73962e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the above model is 1.0\n"
     ]
    }
   ],
   "source": [
    "#Q5: Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
    "#splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
    "# (Use Dataset from sklearn package)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "df = pd.DataFrame( data.data , columns= data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "df = df[df['target'] != 2]\n",
    "\n",
    "x = df.iloc[: , :-1]\n",
    "y = df.iloc[: , -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(x , y , test_size=0.3 , random_state=1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model5 = LogisticRegression(max_iter=400)\n",
    "model5.fit(x_train , y_train)\n",
    "y_pred = model5.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"The accuracy for the above model is {accuracy_score(y_test , y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6aaae0f-d010-4843-83c7-c10a144d2e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30467322 -0.3003339   1.09759147  0.44211871]]\n",
      "The accuracy for the above model is 1.0\n"
     ]
    }
   ],
   "source": [
    "#Q6: Write a Python program to train a Logistic Regression model using L2\n",
    "# regularization (Ridge) and print the model coefficients and accuracy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ridge = LogisticRegression(penalty='l2' , C = 0.1)\n",
    "ridge.fit(x_train , y_train)\n",
    "y_ridge = ridge.predict(x_test)\n",
    "\n",
    "print(ridge.coef_)\n",
    "print(f\"The accuracy for the above model is {accuracy_score(y_test , y_ridge)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbf2a811-0253-48e7-80b2-91b9f3517c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70        91\n",
      "           1       0.80      0.71      0.75       110\n",
      "           2       0.79      0.69      0.74        99\n",
      "\n",
      "    accuracy                           0.73       300\n",
      "   macro avg       0.74      0.73      0.73       300\n",
      "weighted avg       0.74      0.73      0.73       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q7 :Write a Python program to train a Logistic Regression model for multiclass\n",
    "# classification using multi_class='ovr' and print the classification report.\n",
    "# (Use Dataset from sklearn package)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "x , y = make_classification(n_samples=1000 , n_features=10 , n_redundant=5 , n_informative=5 , n_classes=3 , random_state=1)\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.3 , random_state = 23)\n",
    "\n",
    "model8 = LogisticRegression(multi_class='ovr' , solver='lbfgs')\n",
    "\n",
    "model8.fit(x_train , y_train)\n",
    "\n",
    "y_ovr = model8.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test , y_ovr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "299e075f-cc85-4514-bcbe-405cc6c2ccd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "{'C': 1, 'penalty': 'l2'}\n",
      "The accuracy for the above model is 0.73\n"
     ]
    }
   ],
   "source": [
    "#Q8:  Write a Python program to apply GridSearchCV to tune C and penalty\n",
    "# hyperparameters for Logistic Regression and print the best parameters and validation\n",
    "# accuracy.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier = LogisticRegression()\n",
    "params = {\"penalty\":('l1','l2','elasticnet') ,'C' :[1,2,10,20,30,40]}\n",
    "clf = GridSearchCV(classifier , param_grid=params , cv = 5 , verbose=1)\n",
    "clf.fit(x_train , y_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "y_clf = clf.best_estimator_.predict(x_test)\n",
    "\n",
    "print(f\"The accuracy for the above model is {accuracy_score(y_test , y_clf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50ea4b1a-2dfe-4c4a-8d4d-e2a157881f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before scaling is 0.8766666666666667\n",
      "Accuracy after scaling is 0.8766666666666667\n"
     ]
    }
   ],
   "source": [
    "# Q9:  Write a Python program to standardize the features before training Logistic\n",
    "# Regression and compare the model's accuracy with and without scaling.\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "x , y = make_classification(n_samples=1000 , n_features=10 , n_redundant=5,n_informative=5,n_classes=2,random_state=5)\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.3 , random_state = 25)\n",
    "\n",
    "model9 = LogisticRegression(max_iter=350)\n",
    "model9.fit(x_train , y_train)\n",
    "y_simple = model9.predict(x_test)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_fit = scaler.fit_transform(x_train)\n",
    "x_test_fit = scaler.transform(x_test)\n",
    "\n",
    "model91 = LogisticRegression()\n",
    "model91.fit(x_fit , y_train)\n",
    "y_scaled = model91.predict(x_test_fit)\n",
    "\n",
    "print(f\"Accuracy before scaling is {accuracy_score(y_test , y_simple)}\")\n",
    "print(f\"Accuracy after scaling is {accuracy_score(y_test , y_scaled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93123361-98c4-40bc-8fc9-7df24bc4e534",
   "metadata": {},
   "source": [
    "Q10: Imagine you are working at an e-commerce company that wants to\n",
    "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
    "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
    "Logistic Regression model — including data handling, feature scaling, balancing\n",
    "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
    "use case.\n",
    "\n",
    "Ans: To build a robust Logistic Regression model for predicting customer campaign response in an imbalanced 5% positive class scenario, the strategy must prioritize the minority class. We would start by performing feature scaling with StandardScaler and countering imbalance directly in the model using the class_weight='balanced' parameter. Crucially, model evaluation must discard misleading Accuracy and instead focus on Recall (to maximize identified responders, minimizing missed sales) and Precision (to ensure marketing efforts are cost-effective, minimizing wasted budget). The final step is tuning the probability threshold based on the business's tolerance for False Positives versus False Negatives, rather than relying on the default 0.5.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
